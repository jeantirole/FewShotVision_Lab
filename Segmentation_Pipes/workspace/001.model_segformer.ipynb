{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 segformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/anaconda3/envs/trex/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/eric/anaconda3/envs/trex/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/anaconda3/envs/trex/lib/python3.9/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/eric/anaconda3/envs/trex/lib/python3.9/site-packages/transformers/models/segformer/image_processing_segformer.py:102: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Load the pretrained Segformer model\n",
    "segformer_ = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=5\n",
    ")\n",
    "\n",
    "# Extract the encoder\n",
    "encoder = segformer_.segformer.encoder\n",
    "\n",
    "# Example usage: pass inputs to the encoder\n",
    "from transformers import SegformerFeatureExtractor\n",
    "import torch\n",
    "\n",
    "# Load a feature extractor (for preprocessing images)\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/mit-b3\")\n",
    "\n",
    "# Example image tensor (replace with actual preprocessed image data)\n",
    "dummy_image = torch.rand(1, 3, 512, 512)  # Batch of 1, 3 channels, 512x512 resolution\n",
    "\n",
    "# Pass through encoder\n",
    "encoder_outputs = encoder(dummy_image)\n",
    "encoder_hidden_state = encoder_outputs['last_hidden_state']\n",
    "\n",
    "print(encoder_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegformerConfig {\n",
       "  \"_name_or_path\": \"nvidia/mit-b3\",\n",
       "  \"architectures\": [\n",
       "    \"SegformerForImageClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"decoder_hidden_size\": 768,\n",
       "  \"depths\": [\n",
       "    3,\n",
       "    4,\n",
       "    18,\n",
       "    3\n",
       "  ],\n",
       "  \"downsampling_rates\": [\n",
       "    1,\n",
       "    4,\n",
       "    8,\n",
       "    16\n",
       "  ],\n",
       "  \"drop_path_rate\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_sizes\": [\n",
       "    64,\n",
       "    128,\n",
       "    320,\n",
       "    512\n",
       "  ],\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\"\n",
       "  },\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"mlp_ratios\": [\n",
       "    4,\n",
       "    4,\n",
       "    4,\n",
       "    4\n",
       "  ],\n",
       "  \"model_type\": \"segformer\",\n",
       "  \"num_attention_heads\": [\n",
       "    1,\n",
       "    2,\n",
       "    5,\n",
       "    8\n",
       "  ],\n",
       "  \"num_channels\": 3,\n",
       "  \"num_encoder_blocks\": 4,\n",
       "  \"patch_sizes\": [\n",
       "    7,\n",
       "    3,\n",
       "    3,\n",
       "    3\n",
       "  ],\n",
       "  \"reshape_last_stage\": true,\n",
       "  \"semantic_loss_ignore_index\": 255,\n",
       "  \"sr_ratios\": [\n",
       "    8,\n",
       "    4,\n",
       "    2,\n",
       "    1\n",
       "  ],\n",
       "  \"strides\": [\n",
       "    4,\n",
       "    2,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.38.2\"\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- 수정 \n",
    "segformer_.config.num_encoder_blocks = 5\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 128, 320, 512]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.config.hidden_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer_.config.hidden_sizes.append(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 128, 320, 512, 1024]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.config.hidden_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.627416997969522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer_.config.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_ = segformer_(dummy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 128, 128])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 128, 128])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 64, 64])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_.hidden_states[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 32, 32])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_.hidden_states[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 16, 16])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_.hidden_states[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_out = segformer_.decode_head(outputs_.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 128, 128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegformerDecodeHead(\n",
       "  (linear_c): ModuleList(\n",
       "    (0): SegformerMLP(\n",
       "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "    )\n",
       "    (1): SegformerMLP(\n",
       "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
       "    )\n",
       "    (2): SegformerMLP(\n",
       "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
       "    )\n",
       "    (3): SegformerMLP(\n",
       "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Conv2d(768, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.decode_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegformerDecodeHead(\n",
       "  (linear_c): ModuleList(\n",
       "    (0): SegformerMLP(\n",
       "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "    )\n",
       "    (1): SegformerMLP(\n",
       "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
       "    )\n",
       "    (2): SegformerMLP(\n",
       "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
       "    )\n",
       "    (3): SegformerMLP(\n",
       "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Conv2d(768, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.decode_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_ = segformer_.segformer.encoder(dummy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegformerDecodeHead(\n",
       "  (linear_c): ModuleList(\n",
       "    (0): SegformerMLP(\n",
       "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "    )\n",
       "    (1): SegformerMLP(\n",
       "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
       "    )\n",
       "    (2): SegformerMLP(\n",
       "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
       "    )\n",
       "    (3): SegformerMLP(\n",
       "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Conv2d(768, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.decode_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): SegformerLayer(\n",
       "    (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SegformerAttention(\n",
       "      (self): SegformerEfficientSelfAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output): SegformerSelfOutput(\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): SegformerMixFFN(\n",
       "      (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (dwconv): SegformerDWConv(\n",
       "        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      )\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): SegformerLayer(\n",
       "    (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SegformerAttention(\n",
       "      (self): SegformerEfficientSelfAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output): SegformerSelfOutput(\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (drop_path): SegformerDropPath(p=0.003703703870996833)\n",
       "    (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): SegformerMixFFN(\n",
       "      (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (dwconv): SegformerDWConv(\n",
       "        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      )\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): SegformerLayer(\n",
       "    (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SegformerAttention(\n",
       "      (self): SegformerEfficientSelfAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output): SegformerSelfOutput(\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (drop_path): SegformerDropPath(p=0.007407407741993666)\n",
       "    (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): SegformerMixFFN(\n",
       "      (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (dwconv): SegformerDWConv(\n",
       "        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      )\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.segformer.encoder.block[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): SegformerLayer(\n",
       "    (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SegformerAttention(\n",
       "      (self): SegformerEfficientSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (output): SegformerSelfOutput(\n",
       "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (drop_path): SegformerDropPath(p=0.09259259700775146)\n",
       "    (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): SegformerMixFFN(\n",
       "      (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dwconv): SegformerDWConv(\n",
       "        (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "      )\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): SegformerLayer(\n",
       "    (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SegformerAttention(\n",
       "      (self): SegformerEfficientSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (output): SegformerSelfOutput(\n",
       "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (drop_path): SegformerDropPath(p=0.0962962955236435)\n",
       "    (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): SegformerMixFFN(\n",
       "      (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dwconv): SegformerDWConv(\n",
       "        (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "      )\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): SegformerLayer(\n",
       "    (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention): SegformerAttention(\n",
       "      (self): SegformerEfficientSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (output): SegformerSelfOutput(\n",
       "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (drop_path): SegformerDropPath(p=0.10000000149011612)\n",
       "    (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): SegformerMixFFN(\n",
       "      (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dwconv): SegformerDWConv(\n",
       "        (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "      )\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.segformer.encoder.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 16, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hidden = torch.rand(1, 512, 16, 16)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msegformer_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_hidden\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/trex/lib/python3.9/site-packages/transformers/models/segformer/modeling_segformer.py:718\u001b[0m, in \u001b[0;36mSegformerDecodeHead.forward\u001b[0;34m(self, encoder_hidden_states)\u001b[0m\n\u001b[1;32m    713\u001b[0m     encoder_hidden_state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    714\u001b[0m         encoder_hidden_state\u001b[38;5;241m.\u001b[39mreshape(batch_size, height, width, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# unify channel dimension\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m height, width \u001b[38;5;241m=\u001b[39m encoder_hidden_state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[43mencoder_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    719\u001b[0m encoder_hidden_state \u001b[38;5;241m=\u001b[39m mlp(encoder_hidden_state)\n\u001b[1;32m    720\u001b[0m encoder_hidden_state \u001b[38;5;241m=\u001b[39m encoder_hidden_state\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "segformer_.decode_head.forward(dummy_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SegformerDecodeHead.forward of SegformerDecodeHead(\n",
       "  (linear_c): ModuleList(\n",
       "    (0): SegformerMLP(\n",
       "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "    )\n",
       "    (1): SegformerMLP(\n",
       "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
       "    )\n",
       "    (2): SegformerMLP(\n",
       "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
       "    )\n",
       "    (3): SegformerMLP(\n",
       "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Conv2d(768, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_.decode_head.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegformerFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": false,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 512,\n",
       "    \"width\": 512\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dino v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/eric/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/eric/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/eric/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/eric/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "#--- \n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "# Load the processor and model\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#---\n",
      "torch.Size([1, 257, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-large')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs[0]\n",
    "\n",
    "print(\"#---\")\n",
    "print(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"BitImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 256\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_image = torch.rand(1, 3, 256, 256)  # Batch of 1, 3 channels, 512x512 resolution\n",
    "encoder_out = model(dummy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 325, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
